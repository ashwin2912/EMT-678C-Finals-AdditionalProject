{"cells":[{"cell_type":"code","execution_count":1,"id":"ff35ec07","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/12/20 22:56:04 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"]}],"source":["from pyspark.sql import SparkSession\n","import pyspark.sql.functions as F\n","spark = SparkSession.builder.appName(\"gdelt-trends\").getOrCreate()"]},{"cell_type":"code","execution_count":2,"id":"951e733f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/12/20 22:56:24 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","23/12/20 22:56:39 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","23/12/20 22:56:54 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","23/12/20 22:57:09 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","23/12/20 22:57:24 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","23/12/20 22:57:39 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","23/12/20 22:57:54 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","23/12/20 22:58:09 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","23/12/20 22:58:24 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","23/12/20 22:58:39 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","23/12/20 22:58:54 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","23/12/20 22:59:09 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","23/12/20 22:59:24 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","23/12/20 22:59:39 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n","                                                                                \r"]}],"source":["df = spark.read.csv(path=\"gs://dataproc-staging-us-east1-1077714986221-w3r4k84a/table3\",header=True)"]},{"cell_type":"code","execution_count":25,"id":"d3966ab6","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 467:============================================>         (80 + 16) / 97]\r"]},{"name":"stdout","output_type":"stream","text":["+----------+----------------+---------+----------+--------------------+--------------------+\n","|      Date|SourceCommonName|Sentiment|      news|            v2counts|         v2locations|\n","+----------+----------------+---------+----------+--------------------+--------------------+\n","|2023-08-30|     jutarnji.hr|    -6.87|   poverty|AFFECT#2000000#me...|3#Keaton Beach, F...|\n","|2023-08-30|   eastmoney.com|     5.13|epu_policy|SEIZE#2#frame#3#P...|1#China#CH#CH##35...|\n","+----------+----------------+---------+----------+--------------------+--------------------+\n","only showing top 2 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 472:=>           (93 + 8) / 1129][Stage 473:>            (83 + 8) / 1129]\r"]}],"source":["df.show(2)"]},{"cell_type":"code","execution_count":3,"id":"0bdfdf6d","metadata":{},"outputs":[],"source":["from pyspark.sql.functions import col, to_date, mean, count, weekofyear, date_trunc\n","from pyspark.sql import Window\n","\n","def creating_dataset(df, column):\n","    '''\n","    Column is the column in which are allocated the keywords, for every case: political, social and economical columns\n","    '''\n","    # Convert 'Date' column to date type\n","    df = df.withColumn('Date', to_date(col('Date')))\n","\n","    # List of new columns (keywords)\n","    list_keywords = [row[column] for row in df.select(column).distinct().collect()]\n","    df_final = df.select('Date').distinct().withColumnRenamed('Date', 'date')\n","\n","    for k in list_keywords:\n","        # Creating a new dataframe for every keyword in the column, getting the occurrences of keyword and mean of sentiment\n","        df_keyword = df.filter(df[column] == k).groupBy('Date').agg(count('*').alias('count'), mean('Sentiment').alias('mean'))\n","        \n","        # This will be our score, occurrences * mean\n","        df_keyword = df_keyword.withColumn(k, col('count') * col('mean'))\n","\n","        # Date column to perform the join by it\n","        df_keyword = df_keyword.withColumnRenamed('Date', 'date')\n","\n","        # Join with the main dataframe\n","        df_final = df_final.join(df_keyword.select('date', k), on='date', how='left')\n","\n","    # grouping by week\n","    window = Window.orderBy(col('date'))\n","    df_final = df_final.withColumn('date', date_trunc('week', col('date'))).groupBy('date').mean()\n","\n","    for column in df_final.columns:\n","        if column.startswith('avg('):\n","            new_column_name = column[4:-1]  # Remove 'avg(' at start and ')' at the end\n","            df_final = df_final.withColumnRenamed(column, new_column_name)\n","\n","    # Sort by date and reset index\n","    df_final = df_final.orderBy('date')\n","\n","    return df_final"]},{"cell_type":"code","execution_count":4,"id":"fa1ccf88","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["df_gdelt_news = df.orderBy(\"Date\")\n","df_new = creating_dataset(df_gdelt_news,\"news\")"]},{"cell_type":"code","execution_count":null,"id":"029d6c12","metadata":{},"outputs":[],"source":["#Created our feature dataframe by aggregating Theme Sentiment Scores per week by calling the creating_dataset function"]},{"cell_type":"code","execution_count":5,"id":"18d42bba","metadata":{},"outputs":[],"source":["df_new = df_new.withColumnRenamed(\"date\",\"timestamp_date\")\n","df_new = df_new.withColumn(\"date\", col(\"timestamp_date\").cast(\"date\"))\n","df_new = df_new.drop(\"timestamp_date\")"]},{"cell_type":"code","execution_count":7,"id":"bdced177","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/12/20 23:04:22 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","[Stage 108:>                                                        (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+--------------------------------------+-------------------+-------------------+-------------+-------------------+------------------------+------------------+-------------------+-------------------+--------------------------+-------------------+------------------+-------------------+----------+\n","|job_quality_&_labor_market_performance|            poverty|         bankruptcy|central_banks|       stock_market|health_economics_finance|        epu_policy|          oil_price|    economic_growth|financial_arch_and_banking| Debt_Vulnerability|         inflation|    econ_free_trade|      date|\n","+--------------------------------------+-------------------+-------------------+-------------+-------------------+------------------------+------------------+-------------------+-------------------+--------------------------+-------------------+------------------+-------------------+----------+\n","|                    -124.2433333333333| -242.0266666666666|-13.059999999999999|         null|            -399.67|      -7.609999999999999|-11433.40666666667|              -3.71|             -19.22|       -1.9033333333333333| -6.356666666666668|            -4.745|               null|2020-12-28|\n","|                   -109.16285714285712| -328.3342857142857| -6.581666666666667|         null| -787.6914285714287|     -13.956666666666665|-46884.21571428597| -4.743333333333333| -18.31857142857143|       -17.942857142857143|-19.340000000000003|-7.673333333333335|               1.17|2021-01-04|\n","|                   -136.92857142857144|-200.85142857142858|-3.3899999999999992|         null|-455.79857142857117|     -3.3499999999999965|-43587.72142857131|-1.4740000000000002|-14.132857142857143|       -15.342857142857143|-24.372857142857146|            -4.362|-2.3033333333333332|2021-01-11|\n","|                    -85.28000000000002|-286.35285714285703|           -26.4225|         null|-398.34714285714296|                  -16.93|-20168.25714285715|-10.264285714285716|-13.704285714285716|       -11.752857142857142|             -13.24|            -9.486|             -3.285|2021-01-18|\n","|                    -99.95714285714284| -500.8742857142858|-18.493333333333332|        -3.52|-423.99857142857155|     -118.17285714285713| -20197.3228571428|             -1.105| 3.9085714285714253|       -5.6899999999999995| -15.55142857142857|-8.196666666666667| 1.3849999999999998|2021-01-25|\n","+--------------------------------------+-------------------+-------------------+-------------+-------------------+------------------------+------------------+-------------------+-------------------+--------------------------+-------------------+------------------+-------------------+----------+\n","only showing top 5 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["df_new = df_new.orderBy(\"Date\")\n","df_new.show(5)"]},{"cell_type":"code","execution_count":10,"id":"9d556b78","metadata":{},"outputs":[],"source":["df_google = spark.read.csv(path=\"gs://dataproc-staging-us-east1-1077714986221-w3r4k84a/unemployment_pytrends-2020.csv\",header=True)"]},{"cell_type":"code","execution_count":11,"id":"27505e7c","metadata":{},"outputs":[],"source":["df_google = df_google.orderBy(\"date\")\n","df_google_dates = df_google.select(\"date\").distinct()\n","df_google_dates = df_google_dates.withColumn(\"date\", F.to_date(F.col(\"date\"))).orderBy(\"date\")"]},{"cell_type":"code","execution_count":12,"id":"c2c1e23b","metadata":{},"outputs":[],"source":["keyword_list = [row['keyword'] for row in df_google.select('keyword').distinct().collect()]\n","keyword_list.sort()"]},{"cell_type":"code","execution_count":null,"id":"060824bd","metadata":{},"outputs":[],"source":["#Renaming column from trend_index to Unemployment"]},{"cell_type":"code","execution_count":13,"id":"e2644ab2","metadata":{},"outputs":[],"source":["for k in keyword_list:\n","    df_google_dates = df_google_dates.join(df_google.filter(df_google['keyword'] == k).select('date', 'trend_index').withColumnRenamed('trend_index', k), 'date', 'left')"]},{"cell_type":"code","execution_count":14,"id":"f56c121e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- date: date (nullable = true)\n"," |-- unemployment: double (nullable = true)\n","\n"]}],"source":["df_google_dates = df_google_dates.withColumn(\"unemployment\", col(\"unemployment\").cast(\"double\"))\n","df_google_dates.printSchema()"]},{"cell_type":"code","execution_count":26,"id":"2a594421","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 472:=>          (103 + 8) / 1129][Stage 473:=>           (91 + 8) / 1129]\r"]},{"name":"stdout","output_type":"stream","text":["+----------+------------+\n","|      date|unemployment|\n","+----------+------------+\n","|2020-01-05|         5.0|\n","|2020-01-12|         5.0|\n","|2020-01-19|         5.0|\n","|2020-01-26|         5.0|\n","|2020-02-02|         5.0|\n","+----------+------------+\n","only showing top 5 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 472:==>         (253 + 8) / 1129][Stage 473:==>         (248 + 8) / 1129]]\r"]}],"source":["df_google_dates.show(5)"]},{"cell_type":"code","execution_count":null,"id":"e172645b","metadata":{},"outputs":[],"source":["#Manually defining the start and end dates, ideally this should be extracted from the data being considered"]},{"cell_type":"code","execution_count":15,"id":"5467dd60","metadata":{},"outputs":[],"source":["from datetime import datetime, timedelta\n","from pyspark.sql import Row\n","date1 = datetime.strptime('2020-12-28', '%Y-%m-%d')\n","max_date = datetime.strptime('2023-11-30', '%Y-%m-%d')\n","date_list = [date1 + timedelta(days=x) for x in range(0, (max_date - date1).days + 1, 7)]\n","df_f = spark.createDataFrame([Row(date=d) for d in date_list])\n","df_f = df_f.withColumnRenamed(\"date\",\"timestamp_date\")\n","df_f = df_f.withColumn(\"date\", col(\"timestamp_date\").cast(\"date\"))\n","df_f = df_f.drop(\"timestamp_date\")"]},{"cell_type":"code","execution_count":null,"id":"cf99c57d","metadata":{},"outputs":[],"source":["#Joining the features and the prediction column on the date column"]},{"cell_type":"code","execution_count":16,"id":"606ba552","metadata":{},"outputs":[],"source":["df_final = df_f\n","datasets = [df_new,df_google_dates]\n","for d in datasets:\n","    df_final = df_final.join(d,'date','left_outer')"]},{"cell_type":"code","execution_count":17,"id":"21d821d0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- date: date (nullable = true)\n"," |-- job_quality_&_labor_market_performance: double (nullable = true)\n"," |-- poverty: double (nullable = true)\n"," |-- bankruptcy: double (nullable = true)\n"," |-- central_banks: double (nullable = true)\n"," |-- stock_market: double (nullable = true)\n"," |-- health_economics_finance: double (nullable = true)\n"," |-- epu_policy: double (nullable = true)\n"," |-- oil_price: double (nullable = true)\n"," |-- economic_growth: double (nullable = true)\n"," |-- financial_arch_and_banking: double (nullable = true)\n"," |-- Debt_Vulnerability: double (nullable = true)\n"," |-- inflation: double (nullable = true)\n"," |-- econ_free_trade: double (nullable = true)\n"," |-- unemployment: double (nullable = true)\n","\n"]}],"source":["df_final.printSchema()"]},{"cell_type":"code","execution_count":18,"id":"0146a893","metadata":{},"outputs":[],"source":["df_final = df_final.fillna(0)"]},{"cell_type":"code","execution_count":19,"id":"2dec345e","metadata":{},"outputs":[],"source":["from pyspark.ml.feature import VectorAssembler, StandardScaler\n","from pyspark.ml.stat import ChiSquareTest\n","from pyspark.sql.functions import col"]},{"cell_type":"code","execution_count":20,"id":"c6991d91","metadata":{},"outputs":[],"source":["features_cols = [col for col in df_final.columns if col not in [\"date\", \"unemployment\"]]\n","assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\")\n","df_vector = assembler.transform(df_final)"]},{"cell_type":"code","execution_count":21,"id":"7b95ecef","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n","scalerModel = scaler.fit(df_vector)\n","df_scaled = scalerModel.transform(df_vector)"]},{"cell_type":"code","execution_count":23,"id":"041924a5","metadata":{},"outputs":[],"source":["train_df, test_df = df_scaled.randomSplit([0.8, 0.2], seed=1234)"]},{"cell_type":"code","execution_count":22,"id":"5fd2e354","metadata":{},"outputs":[],"source":["from pyspark.ml.regression import LinearRegression\n","lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"unemployment\", regParam=0.1, elasticNetParam=0.5)"]},{"cell_type":"code","execution_count":null,"id":"feafeeac","metadata":{},"outputs":[],"source":["lrModel = lr.fit(train_df)\n","predictions = lrModel.transform(test_df)"]},{"cell_type":"code","execution_count":null,"id":"6445e39c","metadata":{},"outputs":[],"source":["predictions.write.parquet(\"gs://dataproc-staging-us-east1-1077714986221-w3r4k84a/outputs/predictions_b69e.parquet\")"]},{"cell_type":"code","execution_count":null,"id":"a6c7d44b","metadata":{},"outputs":[],"source":["from pyspark.ml.evaluation import RegressionEvaluator\n","evaluator = RegressionEvaluator(labelCol=\"unemployment\", predictionCol=\"prediction\", metricName=\"rmse\")\n","rmse = evaluator.evaluate(predictions)\n","print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"]},{"cell_type":"code","execution_count":null,"id":"29fd284d","metadata":{},"outputs":[],"source":["evaluator.setMetricName(\"mse\")\n","mse = evaluator.evaluate(predictions)\n","print(\"Mean Squared Error (MSE) on test data = %g\" % mse)"]},{"cell_type":"code","execution_count":null,"id":"78b2cdb5","metadata":{},"outputs":[],"source":["evaluator.setMetricName(\"mae\")\n","mae = evaluator.evaluate(predictions)\n","print(\"Mean Absolute Error (MAE) on test data = %g\" % mae)"]},{"cell_type":"code","execution_count":null,"id":"97e74fa1","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}